spring.application.name=big-data-ai
server.port=8888
spring.thymeleaf.cache=false
# for LM Studio
# spring.ai.model.chat=openai
# spring.ai.openai.base-url=http://100.97.9.97:1234/
# spring.ai.openai.api-key=nokey
# spring.ai.openai.chat.options.model=qwen3-coder-480b-a35b-instruct-mlx

# for ollama
# The base URL of your running Ollama instance
spring.ai.ollama.base-url=http://localhost:11434

#spring.ai.ollama.chat.options.model=gemma3-for-demos
#spring.ai.ollama.chat.options.model=gemma3:27b-it-q8_0
spring.ai.ollama.chat.options.model=qwen3-coder:latest
#spring.ai.ollama.chat.options.model=qwen3:8b
#spring.ai.ollama.chat.options.model=qwen2.5:72b

# Optional: You can configure other model parameters like temperature
#spring.ai.ollama.chat.options.temperature=0.1
